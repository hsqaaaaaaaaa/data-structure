Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this seArtificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.cond system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls “reach”, is due to scientific theories being hard to vary. In this work we investigate Deutsch’s hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam’s razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain – the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.